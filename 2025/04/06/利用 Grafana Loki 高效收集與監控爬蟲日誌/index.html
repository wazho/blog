<!DOCTYPE html>
<html>
    <head>
  <meta charset="utf-8">
  <meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, viewport-fit=cover" name="viewport" />
  <meta name="robots" content="index, follow">
  <!-- title -->
  
    
  <title>利用 Grafana Loki 高效收集與監控爬蟲日誌: Salmon</title>
    
  
  
  <!-- open graph -->
  <meta name="description" content="在 Lawsnote 服務期間，因應產品與專案需求，我們需要開發數百支網頁抓取程式 (Web Scraper) 與網頁爬取程式 (Web Crawler)，從政府主管機關的網站中擷取特定法學資料，並在資料梳理的同時，進行結構化處理並寫入資料庫中。  網頁抓取程式 (Web Scraper) 與網頁爬取程式 (Web Crawler)，以下統稱為 crawlers。  Lawsnote 提供的 La">
<meta property="og:type" content="article">
<meta property="og:title" content="利用 Grafana Loki 高效收集與監控爬蟲日誌">
<meta property="og:url" content="https://blog.salmon.tw/2025/04/06/%E5%88%A9%E7%94%A8%20Grafana%20Loki%20%E9%AB%98%E6%95%88%E6%94%B6%E9%9B%86%E8%88%87%E7%9B%A3%E6%8E%A7%E7%88%AC%E8%9F%B2%E6%97%A5%E8%AA%8C/index.html">
<meta property="og:site_name" content="Salmon">
<meta property="og:description" content="在 Lawsnote 服務期間，因應產品與專案需求，我們需要開發數百支網頁抓取程式 (Web Scraper) 與網頁爬取程式 (Web Crawler)，從政府主管機關的網站中擷取特定法學資料，並在資料梳理的同時，進行結構化處理並寫入資料庫中。  網頁抓取程式 (Web Scraper) 與網頁爬取程式 (Web Crawler)，以下統稱為 crawlers。  Lawsnote 提供的 La">
<meta property="og:locale" content="zh_TW">
<meta property="article:published_time" content="2025-04-06T12:00:00.000Z">
<meta property="article:modified_time" content="2025-04-06T05:49:54.876Z">
<meta property="article:author" content="Ze-Hao Wang (Salmon)">
<meta property="article:tag" content="node.js, back-end, full-stack">
<meta name="twitter:card" content="summary">
  <!-- canonical -->
  
  <link rel="canonical" href="https://blog.salmon.tw/2025/04/06/利用 Grafana Loki 高效收集與監控爬蟲日誌/">
  
  <!-- Favicon -->
  <link rel="shortcut icon" href="/img/favicon.ico">
  <link rel="apple-touch-icon" sizes="180x180" href="/img/apple-touch-icon.png">
  <!-- CSS -->
  
<link rel="stylesheet" href="/css/reset.css">

  
<link rel="stylesheet" href="/css/style.css">

  
<link rel="stylesheet" href="/css/markdown.css">

  
<link rel="stylesheet" href="/css/fonts.css">

<meta name="generator" content="Hexo 7.3.0"></head>

    <body>
        <div class="paper">
            <div class="paper-main">
                
                    <div class="post-header">
    <a class="logo" href="/">Salmon</a>
    <!-- <div class="logo"><a href="/" title="Len"><img src="/img/logo.svg" alt="Len" aria-label="logo" height="20"></a></div> -->
        <ul class="nav">
            
            <li><a href="/">Home</a></li>
            
            <li><a target="_blank" rel="noopener" href="https://salmon.tw">About</a></li>
            
            <li><a href="/archives">Archives</a></li>
            
        </ul>


    </a>
</div>

                
                <div class="post-main">
    
        <div class="post-main-title">
            利用 Grafana Loki 高效收集與監控爬蟲日誌
        </div>
        <div class="post-meta">
            2025-04-06 ｜ 
            
        </div>
        <!-- 圆角分类 -->
        <!-- <div class="tags"> -->
            <!--  -->
        <!-- </div> -->
        <div class="post-md">
            <p>在 <a target="_blank" rel="noopener" href="https://about.lawsnote.com/">Lawsnote</a> 服務期間，因應產品與專案需求，我們需要開發數百支網頁抓取程式 (Web Scraper) 與網頁爬取程式 (Web Crawler)，從政府主管機關的網站中擷取特定法學資料，並在資料梳理的同時，進行結構化處理並寫入資料庫中。</p>
<blockquote>
<p>網頁抓取程式 (Web Scraper) 與網頁爬取程式 (Web Crawler)，以下統稱為 <strong>crawlers</strong>。</p>
</blockquote>
<p>Lawsnote 提供的 <a target="_blank" rel="noopener" href="https://lawsnote.com/">Lawsnote Search</a> (法學搜尋引擎)、<a target="_blank" rel="noopener" href="https://blog.lawsnote.com/2022/12/lawsnote-regtech-compliance-solution/">企業法遵系統方案</a>、<a target="_blank" rel="noopener" href="https://monta.lawsnote.com/">Lawsnote Monta</a> (法遵解決方案) 等服務，皆高度講求資料的 <strong>即時性</strong> 與 <strong>準確性</strong>。</p>
<p>因此在我們的團隊中，除了例行性的程式開發外，也必須有效率地管理這數百支 crawlers。當任何一支程式發生錯誤時，能夠即時通知相關人員並迅速定位問題，是我們維運流程中的關鍵。</p>
<p>為了達成這個目標，crawler 的日誌 (Logs) 便需要被妥善地收集與監控。這些日誌不僅包含執行狀態，也涵蓋整個過程中所爬取的請求 URL、HTTP 狀態碼、錯誤訊息 (Error Message) 等重要資訊。</p>
<p>在這篇文章中，我們將介紹如何利用 Grafana Loki 收集與監控這些日誌資料，並最終透過 Grafana 進行可視化展示，打造一個高效、穩定且實用的日誌監控方案。無論你是第一次接觸 Loki，或是想優化排程程式的日誌管理流程，希望這篇文章都能帶來一些實用的參考。</p>
<h2 id="本文架構"><a href="#本文架構" class="headerlink" title="本文架構"></a>本文架構</h2><p>這篇文章將從實務出發，介紹如何在 crawlers 專案中導入 Loki 與 Grafana，實作一套好用的日誌監控方案。主要內容包含：</p>
<ol>
<li><p><strong>為什麼需要日誌監控？</strong><br>描述 crawlers 的管理挑戰，以及 logs 在即時監控與錯誤追蹤中的重要性。</p>
</li>
<li><p><strong>使用 Grafana Loki 收集日誌資料</strong><br>包含 Loki、Promtail 安裝與設定，如何將 crawler logs 傳送進 Loki。</p>
</li>
<li><p><strong>透過 Grafana 可視化監控日誌</strong><br>如何建立 dashboard、撰寫 LogQL 查詢，以及設定警報通知機制。</p>
</li>
<li><p><strong>應用實例與延伸建議</strong><br>實際案例分享與一些擴充應用的建議。</p>
</li>
</ol>
<h2 id="為什麼需要日誌監控？"><a href="#為什麼需要日誌監控？" class="headerlink" title="為什麼需要日誌監控？"></a>為什麼需要日誌監控？</h2><p>當系統規模尚小、程式數量不多時，偵錯階段通常可以靠 <strong>標準輸出 (stdout)</strong> 和標準 <strong>錯誤輸出 (stderr)</strong> 直接在 terminal 或 console 上查看，抑或是將 log 輸出至檔案中。</p>
<p>倘若規模逐步擴大後，若採用相同方式進行管理，將會面臨以下幾個挑戰：</p>
<ol>
<li><p><strong>日誌分散於不同主機或容器中，難以集中查閱</strong><br> 當程式數量一多，可能又分散執行在不同機器上，若沒有集中式的日誌收集機制，維運人員往往需要 SSH 到不同節點去翻找 log，不僅耗時，也容易遺漏資訊。</p>
</li>
<li><p><strong>缺乏統一查詢介面與格式標準，難以追溯問題源頭</strong><br> 每支程式可能使用不同的 log 格式與層級，導致即使 log 都存在，也難以快速篩選出錯誤來源或還原事件順序。</p>
</li>
<li><p><strong>日誌儲存空間有限，歷史紀錄易被覆蓋</strong><br> 若未搭配 <strong>日誌輪替 (log rotation)</strong> 機制，過舊的日誌可能會被覆蓋，導致事後調查時無法取得完整資訊。</p>
</li>
<li><p><strong>錯誤發生無法即時通知相關人員</strong><br> 傳統 log 記錄方式僅作為事後檢查之用，不易即時反饋問題。</p>
</li>
<li><p><strong>難以進行統計與視覺化分析</strong><br> 若想瞭解各程式的整體執行狀況，以 crawler 舉例有 <em>階段成功和失敗資訊</em>、<em>最常失敗的目標網站</em>、<em>HTTP 狀態碼分布</em> 等，將需自行撰寫腳本分析散落的 log，成效不彰。</p>
</li>
</ol>
<h3 id="什麼樣的資料適合收集？"><a href="#什麼樣的資料適合收集？" class="headerlink" title="什麼樣的資料適合收集？"></a>什麼樣的資料適合收集？</h3><p>在設計日誌系統時，並不是所有輸出都適合或需要被收集。我們應該根據系統特性與實際需求，挑選對後續除錯、分析與監控有實質幫助的資料。以開發 crawlers 的經驗來說，建議可以優先考慮以下幾類資訊：</p>
<ol>
<li><p><strong>請求相關資訊</strong><br> 包括 <em>目標 URL</em>、<em>HTTP method</em>、<em>status code</em>、<em>response time</em> 等，能協助判斷目標網站是否正常、回應速度是否合理。</p>
 <figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">2025-01-01 12:00:00 - Requesting URL: https://example.com/api/data</span><br><span class="line">2025-01-01 12:00:01 - Response Status Code: 200</span><br><span class="line">2025-01-01 12:00:01 - Response Time: 200ms</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>任務執行階段狀態</strong><br> 每個任務的開始與結束時間，以及中間執行的步驟 (如：下載 → 解析 → 寫入資料庫) 等，有助於還原執行流程與評估效能瓶頸。</p>
 <figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">2025-01-01 12:00:00 - Task started: Fetching data from example.com</span><br><span class="line">2025-01-01 12:00:01 - Parsing data...</span><br><span class="line">2025-01-01 12:00:02 - Writing data to database...</span><br><span class="line">2025-01-01 12:00:03 - Task completed: Data successfully fetched and stored.</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>錯誤與異常</strong><br> 包括 <em>HTTP 錯誤</em>、<em>timeout</em>、<em>資料解析失敗</em> 等。這類訊息應清楚標示，方便集中查詢與設計告警條件。</p>
 <figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">2025-01-01 12:00:00 - HTTP Error: 404 Not Found</span><br><span class="line">2025-01-01 12:00:00 - Timeout Error: Request to https://example.com/api/data timed out.</span><br><span class="line">2025-01-01 12:00:00 - Parsing Error: Failed to parse JSON response from https://example.com/api/data</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>環境、版本與其他自定義資訊</strong><br> 包括 <em>爬蟲版本</em>、<em>執行節點 IP</em>、<em>執行時間的環境變數</em>、<em>使用的函式庫版本</em> 等，有助於 debug 與還原特定部署狀態。</p>
 <figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">2025-01-01 12:00:00 - Crawler Version: 1.0.0</span><br><span class="line">2025-01-01 12:00:00 - Hostname: crawler-node-01</span><br><span class="line">2025-01-01 12:00:00 - IP Address: 10.0.0.15</span><br><span class="line">2025-01-01 12:00:00 - Node.js Version: v22.11.0</span><br><span class="line">2025-01-01 12:00:00 - Environment: production</span><br><span class="line">2025-01-01 12:00:00 - Dependencies: axios@1.6.8, cheerio@1.0.0-rc.12</span><br></pre></td></tr></table></figure></li>
</ol>
<h2 id="如何設計日誌系統？"><a href="#如何設計日誌系統？" class="headerlink" title="如何設計日誌系統？"></a>如何設計日誌系統？</h2><h3 id="日誌等級"><a href="#日誌等級" class="headerlink" title="日誌等級"></a>日誌等級</h3><p>除了決定「記錄什麼」，另一個關鍵是「以什麼等級記錄」。合理區分 log 等級，不僅能協助後續查詢與視覺化統計，也能減少不必要的雜訊。</p>
<p>以下是常見的 log 等級分類及其適用情境：</p>
<ul>
<li><p><code>debug</code> — 除錯時用的詳細資訊，例如變數值、執行流程細節。<br>  適合開發階段使用，通常在正式環境會關閉或降低權重。</p>
</li>
<li><p><code>info</code> — 代表系統正常執行的重要節點，例如任務啟動、請求成功、寫入資料完成等。<br>  是大多數 log 的主體，可用來觀察系統整體行為。</p>
</li>
<li><p><code>warn</code> — 非預期但尚可接受的狀況，例如重試、資料格式不符但能容錯處理。<br>  可視為潛在異常的訊號。</p>
</li>
<li><p><code>error</code> — 發生錯誤且可能影響任務執行的情況，例如 HTTP 失敗、解析錯誤、寫入失敗等。<br>  應重點監控與通知。</p>
</li>
<li><p><code>fatal</code> 或 <code>critical</code> — 系統無法繼續執行的嚴重錯誤，會中斷整個流程。<br>  可視為需立即介入的事件（有些 logger 可設定觸發終止程序）。</p>
</li>
</ul>
<p>良好的 log 設計不只在於資訊完整，更在於資訊有層次。建議 crawler 在撰寫 log 時，根據情境合理設定等級，並搭配集中式日誌系統進行過濾與查詢。</p>
<h3 id="如何判斷「哪些日誌值得記錄」？"><a href="#如何判斷「哪些日誌值得記錄」？" class="headerlink" title="如何判斷「哪些日誌值得記錄」？"></a>如何判斷「哪些日誌值得記錄」？</h3><p>從前面的章節中，我們可以歸納出一些通用的設計原則，協助判斷哪些日誌值得收集與追蹤：</p>
<ul>
<li><p><strong>可用於判斷系統是否「有在運作」</strong><br>  記錄任務開始、結束、請求發出與回應，是掌握系統存活狀態的基本資訊。這類訊息適合設為 <code>info</code> 等級，作為系統流程的日常記錄依據。</p>
</li>
<li><p><strong>可用於觀察效能瓶頸或趨勢</strong><br>  例如 response time、任務執行時間等，可以透過 log 分析出哪個階段最耗時，協助做後續優化。這類資料也屬於 <code>info</code> 等級，對於後續視覺化有所幫助。</p>
</li>
<li><p><strong>可用於追蹤錯誤的發生條件與上下文</strong><br>  記錄錯誤類型、來源 URL、發生時間與當下的資料，能讓除錯更快速、具體。這類訊息應設定為 <code>error</code> 或 <code>warn</code> 等級，並視嚴重程度觸發警報。</p>
</li>
<li><p><strong>可用於重建問題發生時的執行環境</strong><br>  版本、IP、套件資訊等屬於部署狀態的資訊，是追蹤難以重現的 bug 時的重要依據。這些背景資訊可設定為 <code>info</code> 等級，於啟動時集中記錄一次。</p>
</li>
</ul>
<p>這些原則不僅適用於 crawler 系統，也同樣適用於其他類型的應用系統，例如排程任務（cron jobs）、API server 等，都是日誌設計中值得參考的通則。</p>
<h3 id="可結構化的日誌設計原則"><a href="#可結構化的日誌設計原則" class="headerlink" title="可結構化的日誌設計原則"></a>可結構化的日誌設計原則</h3><p>在實務中，雖然 <code>console.log()</code> 是最簡單快速的日誌方式，但當我們希望日誌能被機器判讀、進行關鍵欄位過濾與分析時，「可結構化」就變得相當重要。</p>
<p>所謂 <strong>可結構化日誌 (Structured Logging)</strong>，指的是以明確欄位命名的格式輸出 log，常見為 JSON 格式。這樣的做法能讓日誌系統 (如 Loki) 更容易針對欄位進行搜尋與 <strong>聚合 (Aggregation)</strong>，提升查詢效率與可視化價值。</p>
<p>例如，若我們在爬取資料過程中發生了錯誤，傳統的 log 記錄方式可能會是：</p>
<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">2025-01-01 12:00:00 - HTTP Error: 404 Not Found</span><br><span class="line">2025-01-01 12:00:00 - Timeout Error: Request to https://example.com/api/data timed out.</span><br><span class="line">2025-01-01 12:00:00 - Parsing Error: Failed to parse JSON response from https://example.com/api/data</span><br></pre></td></tr></table></figure>

<p>以 JSON-based 的格式呈現，會變成：</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">  <span class="attr">&quot;timestamp&quot;</span><span class="punctuation">:</span> <span class="string">&quot;2025-01-01T12:00:00Z&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;level&quot;</span><span class="punctuation">:</span> <span class="string">&quot;error&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;message&quot;</span><span class="punctuation">:</span> <span class="string">&quot;HTTP Error&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;url&quot;</span><span class="punctuation">:</span> <span class="string">&quot;https://example.com/api/data&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;status_code&quot;</span><span class="punctuation">:</span> <span class="number">404</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;error&quot;</span><span class="punctuation">:</span> <span class="string">&quot;Not Found&quot;</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>

        </div>
    
<!-- tags -->

</div>
                <div class="footer">
    <span>Copyright © <script>document.write(new Date().getFullYear())</script> <a target="_blank" rel="noopener" href="https://salmon.tw">Salmon</a></span>
    <!-- <span>Powered by <a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a> with <a target="_blank" rel="noopener" href="https:///imzl.com/zenmind">ZenMind</a></span> -->
</div>

<link rel="stylesheet" href="/css/a11y-dark.min.css">


<script src="/js/highlight.min.js"></script>


<script src="/js/highlightjs-line-numbers.js"></script>

<script>
    hljs.initHighlightingOnLoad();
    hljs.initLineNumbersOnLoad();
</script>

            </div>
        </div>
    </body>
</html>